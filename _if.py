# -*- coding:utf-8 -*- 
import urllib
import urllib2
#### url = "https://passport.baidu.com/v2/?login"
##page = 1
####===============use POST way to login====================
##url = "https://passport.baidu.com/v2/?login"
##values = {"username": "kinsco163@163.com", "password": "XXXX"}
##try:
##    data = urllib.urlencode(values)
##    request = urllib2.Request(url, data)
##    response = urllib2.urlopen(request)
##    print response.read()
##except urllib2.URLError, e:
##    if hasattr(e, "code"):
####        print e.code
##        print 233
##    if hasattr(e, "reason"):
##        print 233
####        print e.reason
##
##
####print response
##
####================use GET way to login====================
##print "use GET way : \n"
##data = urllib.urlencode(values)
##url = "http://passport.csdn.net/account/login"
##url = url + "?" + data
##request = urllib2.Request(url)
##response = urllib2.urlopen(request)
####print data
##
####========================================================
##enable_proxy = True
##proxy_handler = urllib2.ProxyHandler({"http":"http://202.116.76.22:8080"})
##null_proxy_handler = urllib2.ProxyHandler({})
##if enable_proxy:
##    opener = urllib2.build_opener(proxy_handler)
##else:
##    opener = urllib2.build_opener(null_proxy_handler)
##urllib2.install_opener(opener)
##url = "https://passport.csdn.net/account/login"
##
####==============DEBUG LOG==================================
##print 'DEBUG LOG:'
####declare: httpHandler, httpsHandler
####return: opener, response
####method: build_opener, install_opener, urlopen
##httpHandler = urllib2.HTTPHandler(debuglevel=1)
##httpsHandler = urllib2.HTTPSHandler(debuglevel=1)
##opener = urllib2.build_opener(httpHandler, httpsHandler)
##urllib2.install_opener(opener)
##response = urllib2.urlopen('http://www.baidu.com')
##
##===============query gaokao grade==========================
##import requests
##import re
##import cookielib
##url="http://www.sneac.com/gkcjcx/2011zcjcx_jg.jsp?wbtreeid=3063"
##user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
##values={'wbzkzh':'0904150001','wbsfzh':'612422199810290027'}
##header={'User-Agent':user_agent,'Connection':'Keep-alive','Referer': 'http://www.sneac.com/gkcxx/cjcx/zcjcx.htm'}
##data=requests.post(url,data=values,headers=header)
###reg=r'<td\s\w+=.\w+.\w+=.+?>(.*?)</td>'
##reg1=r'<td\s\w+=.\w+.+?>(.*?)</td>'
##data2=re.findall(reg1,data.text)
##print("\n".join(data2))
##================google =========================
##
##url = "http://www.baidu.com"
##request = urllib2.Request(url)
##try:
##    response = urllib2.urlopen(request)
##except urllib2.HTTPError, e:
####    print e.code
####    print e.reason
##    print "heheda"
##except urllib2.URLError, e:
##    print "233"
##else:
##    print 'ok'
####
##cookie = cookielib.CookieJar()
##handler = urllib2.HTTPCookieProcessor(cookie)
##opener = urllib2.build_opener(handler)
##response = opener.open("http://www.baidu.com")
##
##i = 1
##for item in cookie:
##    print str(i) + ' Name = ' + item.name
##    print str(i) + ' Value = ' + item.value
##    i = i + 1

## ====================Cookie====================
##gradeUrl = 'http://jwxt.sdu.edu.cn:7890/pls/wwwbks/bkscjcx.curscopre'
###è¯·æ±‚è®¿é—®æˆç»©æŸ¥è¯¢ç½‘å€
##result = opener.open(gradeUrl)
##print result.read()
##
##filename = "cook.txt"
##cookie = cookielib.MozillaCookieJar(filename)
##urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))
##postdata = urllib.urlencode({
##        'stuid': '2012',
##        'pwd': '2333'
##        })
##loginurl = 'http://jwxt.sdu.edu.cn:7890/pls'
##result = opener.open(loginurl, postdata)
##cookie.save(ignore_discard = True, ignore_expires = True)
##
##================================================
##import cookielib
##filename = 'cookie.txt'
###å£°æ˜ä¸€ä¸ªMozillaCookieJarå¯¹è±¡å®ä¾‹æ¥ä¿å­˜cookieï¼Œä¹‹åå†™å…¥æ–‡ä»¶
##cookie = cookielib.MozillaCookieJar(filename)
###cookie->handler->opener->response
###é€šè¿‡handleræ¥æ„å»ºopener; HTTPCookieProcessor return a handler of cookie
##opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))
###åˆ›å»ºä¸€ä¸ªè¯·æ±‚ï¼ŒåŸç†åŒurllib2çš„urlopen
##response = opener.open("http://www.baidu.com")
###ä¿å­˜cookieåˆ°æ–‡ä»¶
##cookie.save(ignore_discard=True, ignore_expires=True)
##
###åˆ›å»ºMozillaCookieJarå®ä¾‹å¯¹è±¡
####cookie = cookielib.MozillaCookieJar()
#####ä»æ–‡ä»¶ä¸­è¯»å–cookieå†…å®¹åˆ°å˜é‡
####cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)
#####åˆ›å»ºè¯·æ±‚çš„request
##req = urllib2.Request("http://www.baidu.com")
###åˆ©ç”¨urllib2çš„build_openeræ–¹æ³•åˆ›å»ºä¸€ä¸ªopener
##opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))
##response = opener.open(req)
####print response.read()
##import cookielib
####cookie->opener->gradeurl,+result
###å£°æ˜ä¸€ä¸ªMozillaCookieJarå¯¹è±¡å®ä¾‹æ¥ä¿å­˜cookieï¼Œä¹‹åå†™å…¥æ–‡ä»¶
##cookie = cookielib.MozillaCookieJar(filename)
##opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))
##postdata = urllib.urlencode({
##            'stuid':'201200131012',
##            'pwd':'23342321'
##        })
###ç™»å½•æ•™åŠ¡ç³»ç»Ÿçš„URL
##loginUrl = 'http://jwxt.sdu.edu.cn:7890/pls/wwwbks/bks_login2.login'
###æ¨¡æ‹Ÿç™»å½•ï¼Œå¹¶æŠŠcookieä¿å­˜åˆ°å˜é‡
##result = opener.open(loginUrl,postdata)
###ä¿å­˜cookieåˆ°cookie.txtä¸­
####cookie.save(ignore_discard=True, ignore_expires=True)
###åˆ©ç”¨cookieè¯·æ±‚è®¿é—®å¦ä¸€ä¸ªç½‘å€ï¼Œæ­¤ç½‘å€æ˜¯æˆç»©æŸ¥è¯¢ç½‘å€
##gradeUrl = 'http://jwxt.sdu.edu.cn:7890/pls/wwwbks/bkscjcx.curscopre'
###è¯·æ±‚è®¿é—®æˆç»©æŸ¥è¯¢ç½‘å€
##print 'loginUrl:' + loginUrl
##print 'result:' + str(result)
##result = opener.open(gradeUrl)
##print result.read()


##================Scrap Qiushi Wiki using regular expression========================
####'Mozilla/4.0(compatible; MSIE 5.5; Windows NT)'
##import urllib
##import urllib2
##import re
##
##page = 1
##url = 'http://www.qiushibaike.com/hot/page/' + str(page)
##user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
##headers = { 'User-Agent' : user_agent }
##try:
##    request = urllib2.Request(url,headers = headers)
##    response = urllib2.urlopen(request)
##    content = response.read().decode('utf-8')
##    pattern = re.compile('<div.*?author">.*?<a.*?<img.*?>(.*?)</a>.*?<div.*?'+
##                         'content">(.*?)<!--(.*?)-->.*?</div>(.*?)<div class="stats.*?class="number">(.*?)</i>',re.S)
##    items = re.findall(pattern,content)
##    print "hahad"
##    for item in items:
##        haveImg = re.search("img",item[3])
##        if not haveImg:
##            print item[0],item[1],item[2],item[4]
##except urllib2.URLError, e:
##    if hasattr(e,"code"):
##        print e.code
##    if hasattr(e,"reason"):
##        print e.reason

####====start of regular ex===========
##tests = '+20 21.3 22 $0.1 $0 1.2'
##pattern = re.compile('[-+]?[0-9]+(\.[0-9]+)?')
##items = re.findall(pattern, tests)
##for item in items:
##    print item
####=====end of regular ex=============

#============using scraper to scrap Tieba =================
## http://tieba.baidu.com/p/3138733512ï¼Œå‚æ•°éƒ¨åˆ†æ˜¯ ?see_lz=1&pn=13138733512
import re

#å¤„ç†é¡µé¢æ ‡ç­¾ç±»
class Tool:
    #å»é™¤imgæ ‡ç­¾,7ä½é•¿ç©ºæ ¼
    removeImg = re.compile('<img.*?>| {7}|')
    #åˆ é™¤è¶…é“¾æ¥æ ‡ç­¾
    removeAddr = re.compile('<a.*?>|</a>', re.M)
    #æŠŠæ¢è¡Œçš„æ ‡ç­¾æ¢ä¸º\n
    replaceLine = re.compile('<tr(.*?)>|<div(.*?)>|</div>|</p>', re.M)
    #å°†è¡¨æ ¼åˆ¶è¡¨<td>æ›¿æ¢ä¸º\t
    replaceTD= re.compile('<td>')
    #æŠŠæ®µè½å¼€å¤´æ¢ä¸º\nåŠ ç©ºä¸¤æ ¼
    replacePara = re.compile('<p.*?>')
    #å°†æ¢è¡Œç¬¦æˆ–åŒæ¢è¡Œç¬¦æ›¿æ¢ä¸º\n
    replaceBR = re.compile('<br><br>|<br>')
    #å°†å…¶ä½™æ ‡ç­¾å‰”é™¤
    removeExtraTag = re.compile('<.*?>')
    def replace(self,x):
        x = re.sub(self.removeImg,"",x)
        x = re.sub(self.removeAddr,"",x)
        x = re.sub(self.replaceLine,"\n",x)
        x = re.sub(self.replaceTD,"\t",x)
        x = re.sub(self.replacePara,"\n    ",x)
        x = re.sub(self.replaceBR,"\n",x)
        x = re.sub(self.removeExtraTag,"",x)
        #strip()å°†å‰åå¤šä½™å†…å®¹åˆ é™¤
        return x.strip()


#ç™¾åº¦è´´å§çˆ¬è™«ç±»
class BDTB:

    #åˆå§‹åŒ–ï¼Œä¼ å…¥åŸºåœ°å€ï¼Œæ˜¯å¦åªçœ‹æ¥¼ä¸»çš„å‚æ•°
    def __init__(self,baseUrl,seeLZ,floorTag):
        #baseé“¾æ¥åœ°å€
        self.baseURL = baseUrl
        #æ˜¯å¦åªçœ‹æ¥¼ä¸»
        self.seeLZ = '?see_lz='+str(seeLZ)
        #HTMLæ ‡ç­¾å‰”é™¤å·¥å…·ç±»å¯¹è±¡
        self.tool = Tool()
        #å…¨å±€fileå˜é‡ï¼Œæ–‡ä»¶å†™å…¥æ“ä½œå¯¹è±¡
        self.file = None
        #æ¥¼å±‚æ ‡å·ï¼Œåˆå§‹ä¸º1
        self.floor = 1
        #é»˜è®¤çš„æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰æˆåŠŸè·å–åˆ°æ ‡é¢˜çš„è¯åˆ™ä¼šç”¨è¿™ä¸ªæ ‡é¢˜
        self.defaultTitle = u"ç™¾åº¦è´´å§"
        #æ˜¯å¦å†™å…¥æ¥¼åˆ†éš”ç¬¦çš„æ ‡è®°
        self.floorTag = floorTag
        self.title = ''

    #ä¼ å…¥é¡µç ï¼Œè·å–è¯¥é¡µå¸–å­çš„ä»£ç 
    def getPage(self,pageNum):
        try:
            #æ„å»ºURL
            url = self.baseURL+ self.seeLZ + '&pn=' + str(pageNum)
            request = urllib2.Request(url)
            response = urllib2.urlopen(request)
            #è¿”å›UTF-8æ ¼å¼ç¼–ç å†…å®¹
            # print u'\r\nget page: æˆ‘æ˜¯é¡µé¢'
            return response.read().decode('utf-8')
        #æ— æ³•è¿æ¥ï¼ŒæŠ¥é”™
        except urllib2.URLError, e:
            if hasattr(e,"reason"):
                print u"è¿æ¥ç™¾åº¦è´´å§å¤±è´¥,é”™è¯¯åŸå› ",e.reason
                return None

    #è·å–å¸–å­æ ‡é¢˜
    def getTitle(self,page):
        #å¾—åˆ°æ ‡é¢˜çš„æ­£åˆ™è¡¨è¾¾å¼
        pattern = re.compile('<h[1-3] class="core_title_txt.*?>(.*?)</h[1-3]>',re.S)
        result = re.search(pattern,page)
        if result:
            #å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›æ ‡é¢˜
            # print u'\r\nTitle :æˆ‘æ˜¯æ ‡é¢˜'
            return result.group(1).strip()
        else:
            return None

    #è·å–å¸–å­ä¸€å…±æœ‰å¤šå°‘é¡µ
    def getPageNum(self,page):
        ##debug for number
        #è·å–å¸–å­é¡µæ•°çš„æ­£åˆ™è¡¨è¾¾å¼
        pattern = re.compile('<li class="l_reply_num.*?</span>.*?<span.*?>(.*?)</span>',re.S)
        result = re.search(pattern,page)
        if result:
            # print u'\r\nget page num æˆ‘æ˜¯é¡µå·'
            return result.group(1).strip()
        else:
            return None

    #è·å–æ¯ä¸€å±‚æ¥¼çš„å†…å®¹,ä¼ å…¥é¡µé¢å†…å®¹
    def getContent(self,page):
        #åŒ¹é…æ‰€æœ‰æ¥¼å±‚çš„å†…å®¹
        pattern = re.compile('<div id="post_content_.*?>(.*?)</div>',re.S)
        items = re.findall(pattern,page)
        contents = []
        for item in items:
            #å°†æ–‡æœ¬è¿›è¡Œå»é™¤æ ‡ç­¾å¤„ç†ï¼ŒåŒæ—¶åœ¨å‰ååŠ å…¥æ¢è¡Œç¬¦
            content = "\n"+self.tool.replace(item)+"\n"
            contents.append(content.encode('utf-8'))
        # print 'get content æˆ‘æ˜¯å†…å®¹'
        return contents

    def setFileTitle(self,title):
        #å¦‚æœæ ‡é¢˜ä¸æ˜¯ä¸ºNoneï¼Œå³æˆåŠŸè·å–åˆ°æ ‡é¢˜
        if title is not None:
            self.title = title + ".txt"
            self.file = open(title + ".txt","w+")
        else:
            self.title = self.defaultTitle + ".txt"
            self.file = open(self.defaultTitle + ".txt","w+")

    def writeData(self,contents):
        #å‘æ–‡ä»¶å†™å…¥æ¯ä¸€æ¥¼çš„ä¿¡æ¯
        for item in contents:
            if self.floorTag == 1:
                #æ¥¼ä¹‹é—´çš„åˆ†éš”ç¬¦
                floorLine = "\n" + str(self.floor) + u"-----------------------------------------------------------------------------------------\n"
                self.file.write(floorLine)
            self.file.write(item)
            self.floor += 1

    def start(self):
        indexPage = self.getPage(1)
##        print indexPage
        pageNum = self.getPageNum(indexPage)
        title = self.getTitle(indexPage)
        self.setFileTitle(title)
        if pageNum == None:
            print "URLå·²å¤±æ•ˆï¼Œè¯·é‡è¯•"
            return
        try:
            print "è¯¥å¸–å­å…±æœ‰" + str(pageNum) + "é¡µ"
            for i in range(1,int(pageNum)+1):
                print "æ­£åœ¨å†™å…¥ç¬¬" + str(i) + "é¡µæ•°æ®"
                page = self.getPage(i)
                contents = self.getContent(page)
                self.writeData(contents)
        #å‡ºç°å†™å…¥å¼‚å¸¸
        except IOError,e:
            print "å†™å…¥å¼‚å¸¸ï¼ŒåŸå› " + e.message
        finally:
            print u"å†™å…¥ä»»åŠ¡å®Œæˆ"

##3138733512 ,4656381000, 4662226770ï¼ˆä¸­å±±å¤§å­¦ï¼‰
print u"è¯·è¾“å…¥å¸–å­ä»£å·"
baseURL = 'http://tieba.baidu.com/p/' + str(raw_input(u'http://tieba.baidu.com/p/')).strip()
##seeLZ = raw_input("æ˜¯å¦åªè·å–æ¥¼ä¸»å‘è¨€ï¼Œæ˜¯è¾“å…¥1ï¼Œå¦è¾“å…¥0\n")
##floorTag = raw_input("æ˜¯å¦å†™å…¥æ¥¼å±‚ä¿¡æ¯ï¼Œæ˜¯è¾“å…¥1ï¼Œå¦è¾“å…¥0\n")
seeLZ = 0
floorTag = 1
su = u'å“ˆå“ˆ'
bdtb = BDTB(baseURL,seeLZ,floorTag)
bdtb.start()
print 'write to file:' + bdtb.title
bdtb.file.close()
### pycharm Bug:
### http://stackoverflow.com/questions/37010146/how-to-properly-use-a-web-link-as-a-raw-input-in-python/37010347#37010347

##===========self test=========
### -*- coding:utf-8 -*-
##print u"è¯·è¾“å…¥å¸–å­ä»£å·"
##print "è¯·è¾“å…¥å¸–å­ä»£å·"
##print u"å¸–å­ä»£å·".encode('utf-8')##ç”¯æ §ç“™æµ ï½…å½¿
##print u"å¸–å­ä»£å·".encode('gbk')
##print "å¸–å­ä»£å·"                 ##ç”¯æ §ç“™æµ ï½…å½¿
##print u'å¸–å­ä»£å·'.encode('gb2312')
##print u"éå æ±".encode('utf-8')
